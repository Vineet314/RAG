{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ce4967",
   "metadata": {},
   "source": [
    "## Expermient with RAG offered by Google GenAI (Gemini API)\n",
    "\n",
    "This notebook demonstrates how to ask questions about multiple PDFs using Gemini API.\\\n",
    "In this notebook, the PDFs are passed in as file paths. PDFs from the web can also be used.\\\n",
    "This code draws inspiration from the [Google Gemini API documentation](https://developers.generativeai.google/api/rest/v1alpha/gemini.projects.locations.models/chat/completions).\n",
    "\n",
    "Before running this notebook, ensure you have `google-genai` installed. You can install it using pip:\n",
    "\n",
    "`pip install google-genai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddffcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from pathlib import Path\n",
    "import os, io\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter file paths here\n",
    "path1 = Path(r\"../../data/Papers/2501.12948v1DeepSeek R1.pdf\")\n",
    "path2 = Path(r\"../../data/Papers/DeepSeek_V3.pdf\")\n",
    "\n",
    "# Retrieve and upload both PDFs using the File API\n",
    "doc_data_1 = io.BytesIO(path1.read_bytes())\n",
    "doc_data_2 = io.BytesIO(path2.read_bytes())\n",
    "\n",
    "sample_pdf_1 = client.files.upload(\n",
    "  file=doc_data_1,\n",
    "  config=dict(mime_type='application/pdf'))\n",
    "\n",
    "sample_pdf_2 = client.files.upload(\n",
    "  file=doc_data_2,\n",
    "  config=dict(mime_type='application/pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc70efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What are the key differences between DeepSeek-V3 and DeepSeek-R1 in the fine-tuning methods?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.5-flash\",\n",
    "  contents=[sample_pdf_1, sample_pdf_2, prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cbf519e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The key differences in fine-tuning methods between DeepSeek-V3 and DeepSeek-R1 lie primarily in their **objectives, initial training steps, and the source/role of reasoning data**:\n",
       "\n",
       "1.  **Primary Objective:**\n",
       "    *   **DeepSeek-R1:** The core goal is to **incentivize and enhance reasoning capabilities** in LLMs. DeepSeek-R1 (and its precursor DeepSeek-R1-Zero) is explicitly designed and trained to excel at complex reasoning tasks through reinforcement learning.\n",
       "    *   **DeepSeek-V3:** Its post-training goal is broader: to **align the powerful pre-trained base model (DeepSeek-V3-Base) with human preferences and unlock its potential across diverse tasks**, including general capabilities (creative writing, role-playing), factual QA, and reasoning.\n",
       "\n",
       "2.  **Initial State / Cold Start:**\n",
       "    *   **DeepSeek-R1-Zero:** Starts with a base model and applies **pure RL without any supervised fine-tuning (SFT) as a preliminary step**. This is a key distinguishing feature, aiming for reasoning capabilities to emerge naturally.\n",
       "    *   **DeepSeek-R1 (the final model):** Incorporates a **small amount of \"cold-start\" SFT data** (long Chain-of-Thought (CoT) examples) to fine-tune the DeepSeek-V3-Base model *before* its main RL stages. This helps stabilize the early RL phase and improve readability.\n",
       "    *   **DeepSeek-V3:** Undergoes a comprehensive **Supervised Fine-Tuning (SFT) stage** on 1.5 million instances spanning multiple domains. Crucially, the **reasoning data for DeepSeek-V3's SFT is generated by an \"internal DeepSeek-R1 model\"** (via rejection sampling). This means DeepSeek-V3 *learns* reasoning patterns from DeepSeek-R1 rather than developing them from scratch itself during its fine-tuning.\n",
       "\n",
       "3.  **Reinforcement Learning (RL) Stages & Reward Models:**\n",
       "    *   **DeepSeek-R1:**\n",
       "        *   Uses a **multi-stage RL pipeline**.\n",
       "        *   Relies primarily on **rule-based reward systems** for reasoning tasks (math, code), which verify correctness and enforce structural formats (`<think>`, `<answer>` tags).\n",
       "        *   Introduces a **language consistency reward** during RL to address language mixing issues observed in DeepSeek-R1-Zero.\n",
       "        *   The RL process is focused on *deepening* the model's reasoning thought process.\n",
       "    *   **DeepSeek-V3:**\n",
       "        *   Also employs **RL stages** after SFT.\n",
       "        *   Uses a combination of **rule-based Reward Models** (for deterministic tasks) and **model-based Reward Models** (for free-form answers, where DeepSeek-V3 itself can act as a generative reward model to judge outputs). This allows for alignment on a wider range of open-ended tasks beyond just reasoning.\n",
       "        *   The RL helps to refine both reasoning and general helpfulness/harmlessness.\n",
       "\n",
       "4.  **Role in Data Generation/Distillation:**\n",
       "    *   **DeepSeek-R1:** Acts as a **teacher model** for reasoning. The high-quality reasoning data, particularly CoT trajectories, generated by DeepSeek-R1 (especially after its reasoning-oriented RL) is used to distill knowledge into smaller models and also into DeepSeek-V3 itself during its SFT stage.\n",
       "    *   **DeepSeek-V3:** **Consumes reasoning data distilled from DeepSeek-R1**. Its fine-tuning process leverages the reasoning capabilities developed by R1, effectively incorporating them into a more general-purpose model.\n",
       "\n",
       "In essence, DeepSeek-R1 is the specialist, meticulously trained via RL to achieve cutting-edge reasoning. DeepSeek-V3 is the generalist, which then incorporates the specialized reasoning knowledge from DeepSeek-R1 during its own post-training SFT and RL phases, alongside other general-purpose alignment objectives."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de15580b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
