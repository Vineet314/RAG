{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fac82d2a",
   "metadata": {},
   "source": [
    "This notebook is a collection of functions and code snipptes that can be used for handling the pdf files and creating the knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865579ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits a long text into smaller, overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to be chunked.\n",
    "        chunk_size: The desired size of each chunk.\n",
    "        chunk_overlap: The number of characters to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - chunk_overlap\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bd39a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n    Splits a long t',\n",
       " 'ong text into smalle',\n",
       " 'maller, overlapping ',\n",
       " 'ping chunks.\\n\\n    Ar',\n",
       " '   Args:\\n        tex']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Usage\n",
    "text =\"\"\"\n",
    "    Splits a long text into smaller, overlapping chunks.\n",
    "\n",
    "    Args:\n",
    "        text: The input text to be chunked.\n",
    "        chunk_size: The desired size of each chunk.\n",
    "        chunk_overlap: The number of characters to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        A list of text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "chunk_text(text,20,5)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd123ae",
   "metadata": {},
   "source": [
    "Now to extract data from pdf files and create a knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765de82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz, os\n",
    "data_dir = r\"..\\data\\Papers\"\n",
    "knowledge_file = r\"..\\data\\knowledge.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ae132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(papers_dir: str, out_path: str) -> str:\n",
    "    all_text = \"\"\n",
    "    for file in os.listdir(papers_dir):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            filepath = os.path.join(papers_dir, file)\n",
    "            doc = fitz.open(filepath)\n",
    "            for page in doc:\n",
    "                all_text += page.get_text()\n",
    "            doc.close()\n",
    "\n",
    "    assert out_path.endswith(\".txt\"), \"Can only save to a text file\"\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a25b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_text(papers_dir=data_dir, out_path=knowledge_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e8ddac",
   "metadata": {},
   "source": [
    "After the text is extarcted from the pdf files into `knowledge_file`, it can be chunked into smaller pieces and stored in a vector database like ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d0b508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770730"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open(knowledge_file, mode='r',encoding='utf-8').read()\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f40a356a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = chunk_text(text, chunk_size=3500, chunk_overlap=300)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083044f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "oclient = OpenAI(api_key=os.getenv(\"API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac76f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks2embs(chunks:list[str]) -> list[list[float]]:\n",
    "\n",
    "    if (s := sum(len(i) for i in chunks) / 3.5) > 300000:\n",
    "        n = int(np.ceil(s/300000))\n",
    "        k, m = divmod(len(chunks), n)\n",
    "        chunks_ = [chunks[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n)]\n",
    "\n",
    "    embds = []\n",
    "    for hehe in chunks_:\n",
    "        resp = oclient.embeddings.create(\n",
    "            input=hehe,\n",
    "            model=\"text-embedding-3-small\")\n",
    "        \n",
    "        embds.extend([resp.data[i].embedding for i in range(len(hehe))])\n",
    "\n",
    "    return np.array(embds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "069d96a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = chunks2embs(chunks)\n",
    "# x.shape\n",
    "# np.save(r\"../data/embds.npy\", x)\n",
    "embds = np.load(r\"../data/embds.npy\", 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba450a",
   "metadata": {},
   "source": [
    "Now that we have the chunks, persist them in a vector database like ChromaDB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475dd724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "db_dir = r\"../chromadb\"\n",
    "os.makedirs(db_dir, exist_ok=True)\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=db_dir)\n",
    "collection = chroma_client.create_collection(name=\"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07b56759",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(ids=[f'chunk{i}' for i in range(len(chunks))],\n",
    "               documents=chunks,\n",
    "               embeddings=embds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc355da",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is deepseek V3?\"\n",
    "query_emb = oclient.embeddings.create(input=query, model=\"text-embedding-3-small\").data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b1c3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = collection.query(\n",
    "    query_embeddings=query_emb,\n",
    "    n_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79a26d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
